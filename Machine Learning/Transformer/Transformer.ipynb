{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zoEi7-T4dYWC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Encoder part:***\n",
        "\n",
        "\n",
        "```\n",
        "      1.Multihead attention\n",
        "      2.Positional encoding\n",
        "      3.layer normalization\n",
        "      4.Feed Forward Neural Network\n",
        "\n",
        "      1,2,3,4 --> encoding layer\n",
        "\n",
        "      encoder --> num_encodinglayer >=1\n",
        "\n"
      ],
      "metadata": {
        "id": "qZnai4qfdamk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multihead Attention**\n",
        "```\n",
        "linear transformation of input into query , key , value vectors.\n",
        "split it for each heads.\n",
        "scaled dot product attention in each head parallelly.\n",
        "concatenate all attention vectors.\n",
        "linear transform it.\n",
        "\n",
        "Goal is to obtain attention matrix.\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F5CwMueBeJYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "      def __init__(self, input_dim, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(input_dim , 3*d_model) #linear layer for query,key,value\n",
        "        self.linear_layer = nn.Linear(d_model , d_model)#linear layer for final transformation\n",
        "\n",
        "        assert d_model % num_heads ==0, \"d_model must be divisible by num_heads!\"\n",
        "\n",
        "      def __Softmax(self, x): #prefixed with __ to prevent from calling object.softmax\n",
        "          epsilon = 1e-9 #adding small epsilon so that denominator doesnt become zero and the final value doesnt becomes nan!\n",
        "          x0 = x - torch.max(x, dim = -1, keepdims = True)[0]# x - max(x) for numerical stability\n",
        "          return torch.softmax(x0 + epsilon, dim = -1)\n",
        "\n",
        "\n",
        "      def __Scaled_Dot_Product_Attention(self, q, k, v, mask = False):\n",
        "          d_k = k.size(-1)\n",
        "          qk = torch.matmul(q, k.transpose(-2,-1))/torch.sqrt(torch.tensor(d_k, dtype = torch.float32)) #torch.sqrt requires tensor format data\n",
        "          if mask == True:\n",
        "              mask_matrix = torch.tril(torch.ones(qk.size(-2),qk.size(-1)),1)\n",
        "              mask_matrix = mask_matrix.to(qk.device) #setting mask matrix to be in the same device as the qk matrix. device = either cpu,gpu or memory\n",
        "              qk = qk + (1.0 - mask_matrix) * -1e9 #matrix inversion and largely negative value instead of -inf\n",
        "          attention_weights = self.__Softmax(qk)\n",
        "          attention_matrix = torch.matmul(attention_weights, v)\n",
        "          return attention_matrix, attention_weights\n",
        "\n",
        "      def forward(self , x , mask = False):\n",
        "          batch_size , seq_len , input_dim = x.size()\n",
        "          #print(f\"x shape:{x.size()}\")\n",
        "          qkv = self.qkv_layer(x)\n",
        "          #print(f\"qkv shape:{qkv.size()}\")\n",
        "          qkv = qkv.reshape(batch_size , seq_len , self.num_heads , 3*self.head_dim)\n",
        "          #print(f\"qkv shape after splitting heads:{qkv.size()}\")\n",
        "          qkv = qkv.permute(0,2,1,3)\n",
        "          #print(f\"qkv shape after permuting seq_len and num_heads:{qkv.size()}\")\n",
        "          q,k,v = qkv.chunk(3 , dim=-1)\n",
        "         # print(f\"q shape:{q.size()} \\nk shape:{k.size()} \\nv shape():{v.size()}\")\n",
        "          if mask not in [True,False,0,1]:\n",
        "            mask = False # defaulting mask = False if invalid value for mask.\n",
        "          if mask == True:\n",
        "              attention, atn_weights = self.__Scaled_Dot_Product_Attention(q, k, v, mask = True)\n",
        "          else:\n",
        "              attention, atn_weights = self.__Scaled_Dot_Product_Attention(q, k, v)\n",
        "          #print(f\"attention shape:{attention.size()}\")\n",
        "          attention = attention.reshape(batch_size, seq_len, self.num_heads * self.head_dim)\n",
        "          #print(f\"attention shape after concatenation:{attention.size()}\")\n",
        "          output = self.linear_layer(attention)\n",
        "          return output\n",
        "\n",
        "      def forward_cross_attention(self, q, k, v, mask = False):\n",
        "          batch_size, seq_len, _ = q.size()\n",
        "          if mask not in [True,False,0,1]:\n",
        "              mask = False # defaulting mask = False if invalid value for mask.\n",
        "          if mask == True:\n",
        "              attention, atn_weights = self.__Scaled_Dot_Product_Attention(q, k, v, mask = True)\n",
        "          else:\n",
        "              attention, atn_weights = self.__Scaled_Dot_Product_Attention(q, k, v)\n",
        "          #print(f\"attention shape:{attention.size()}\")\n",
        "          attention = attention.reshape(batch_size, seq_len, self.num_heads * self.head_dim) #Concatenation\n",
        "          #print(f\"attention shape after concatenation:{attention.size()}\")\n",
        "          output = self.linear_layer(attention)\n",
        "          return output\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IKRcqlmweIqc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional Encoding**\n",
        "\n",
        "```\n",
        "create a positional encoding matrix using the input embedding and add with the input embedding\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "aiDpkfwD4LMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "      def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "\n",
        "      def __padding(self,x, max_seq_len):#padding the input embedding\n",
        "        padded = torch.zeros((x.size(0), max_seq_len, x.size(2)))\n",
        "        for i, seq in enumerate(x):\n",
        "          padded[i,:len(seq)] = seq #padding if each sequences are of different length, but this is inefficient for large datasets because of looping\n",
        "        return padded\n",
        "\n",
        "\n",
        "      def forward(self, x):\n",
        "          d_model = self.input_dim\n",
        "          max_seq_len = max(len(seq) for seq in x)\n",
        "          x = self.__padding(x, max_seq_len)\n",
        "          positional_matrix = torch.zeros(max_seq_len, d_model)\n",
        "          for pos in range(max_seq_len):\n",
        "            for i in range(0,d_model,2):\n",
        "              positional_matrix[pos][i] = torch.sin(pos/torch.pow(torch.tensor(10000), i/d_model))\n",
        "              if i+1 < d_model:\n",
        "                positional_matrix[pos][i+1] = torch.cos(pos/torch.pow(torch.tensor(10000), i/d_model))\n",
        "\n",
        "          #print(positional_matrix.size())\n",
        "          pos_encoded_matrix = x + positional_matrix\n",
        "          return pos_encoded_matrix"
      ],
      "metadata": {
        "id": "94AP8Jvy4gDe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Addition and layer normalization**\n",
        "\n",
        "```\n",
        "the output of the attention layer and the data from residual/skip connection\n",
        "is added and normalized\n",
        "```"
      ],
      "metadata": {
        "id": "3vPbtfe1OcZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNorm(nn.Module):\n",
        "    def __init__(self, d_model, epsilon = 1e-9):\n",
        "      super().__init__()\n",
        "      self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "      self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "      self.eps = epsilon\n",
        "\n",
        "    def __normalize(self,x):\n",
        "      mean = x.mean(dim = -1, keepdims = True)\n",
        "      var = x.var(dim = -1, keepdims = True, correction = 0) #correction=0 to disable bessel's correction(divide by n-1 instead of).\n",
        "      norm = (x - mean)/torch.sqrt(var + self.eps)\n",
        "      return norm\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "      assert x1.size()==x2.size(),\"dimensionality of attention output and residual connection data should be strictly same!\"\n",
        "      return self.gamma * self.__normalize(x1 + x2) + self.beta\n"
      ],
      "metadata": {
        "id": "7Fq1_6vRAuZ1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feed Forward Neural Network**\n",
        "```\n",
        "transform the data to higher dimension to capture complex information.\n",
        "relu activation to introduce non-linearity.\n",
        "transform back to lower dimension of d_model.\n",
        "```"
      ],
      "metadata": {
        "id": "hzxsDRxneYMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "  def __init__(self,d_model, d_ffn):\n",
        "    super().__init__()\n",
        "    self.d_ffn = d_ffn\n",
        "    self.d_model = d_model\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(d_model, d_ffn),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(d_ffn, d_model)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.ffn(x)\n"
      ],
      "metadata": {
        "id": "JB8uQmQVXMbI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding layer**\n",
        "```\n",
        "structuring the\n",
        "1.positional encoding\n",
        "2.multihead attention\n",
        "3.add and normalization\n",
        "4.feed forward neural network\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "Y9X57ICKfksG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncodingLayer(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, num_heads, d_ffn):\n",
        "      super().__init__()\n",
        "      self.input_dim = input_dim\n",
        "      self.d_model = d_model\n",
        "      self.num_heads = num_heads\n",
        "      self.d_ffn = d_ffn\n",
        "\n",
        "      self.positional_encoder = PositionalEncoding(input_dim = self.input_dim)\n",
        "      self.multihead_attention = MultiHeadAttention(input_dim = self.input_dim, d_model = self.d_model, num_heads = self.num_heads)\n",
        "      self.add_norm1 = AddNorm(d_model = self.d_model)\n",
        "      self.ff_network = FeedForwardNetwork(d_model = self.d_model, d_ffn = self.d_ffn)\n",
        "      self.add_norm2 = AddNorm(d_model = self.d_model)\n",
        "\n",
        "    def forward(self, x, mask = False):\n",
        "      \"\"\" x ---> Input Embedding \"\"\"\n",
        "      x = self.positional_encoder.forward(x)\n",
        "      mha_x = self.multihead_attention.forward(x , mask)\n",
        "      addnorm1 = self.add_norm1.forward(x1 = x, x2 = mha_x)\n",
        "      ffnn_x = self.ff_network.forward(x = addnorm1)\n",
        "      encoder_output = self.add_norm2.forward(x1 = addnorm1, x2 = ffnn_x)\n",
        "      return encoder_output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NDReUwc1e8k9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer Encoder**\n",
        "\n",
        "\n",
        "one or more encoder layers"
      ],
      "metadata": {
        "id": "xHdUit4zYd9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_enc_layers, input_dim, d_model, num_heads, d_ffn):\n",
        "      super().__init__()\n",
        "\n",
        "      self.layers = nn.ModuleList([EncodingLayer(input_dim, d_model, num_heads, d_ffn) for _ in range(num_enc_layers)])\n",
        "\n",
        "    def forward(self, x, mask = False):\n",
        "      for layer in self.layers:\n",
        "          x = layer(x , mask)\n",
        "      return x"
      ],
      "metadata": {
        "id": "AL2WB7hlL1mL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder layer**\n",
        "```\n",
        "structuring:\n",
        "1. Positional encoder for decoder input.\n",
        "2. multihead attention with mask\n",
        "3. layer normalization.\n",
        "4. multihead attention without mask with encoder output and decoder data as input\n",
        "5. layer normalization\n",
        "6. feed forward network\n",
        "7. layer normalization.\n"
      ],
      "metadata": {
        "id": "EYEW35mmbsEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecodingLayer(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, num_heads, d_ffn):\n",
        "      super().__init__()\n",
        "      self.positional_encoder = PositionalEncoding(input_dim)\n",
        "      self.self_attention = MultiHeadAttention(input_dim, d_model, num_heads)\n",
        "      self.add_norm1 = AddNorm(d_model)\n",
        "      self.cross_attention = MultiHeadAttention(input_dim, d_model, num_heads)\n",
        "      self.add_norm2 = AddNorm(d_model)\n",
        "      self.ff_network = FeedForwardNetwork(d_model, d_ffn)\n",
        "      self.add_norm3 = AddNorm(d_model)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask = False, tgt_mask = False):\n",
        "        x = self.positional_encoder(x)\n",
        "        mha_x = self.self_attention(x, tgt_mask)\n",
        "        addnorm1 = self.add_norm1(x, mha_x)\n",
        "        cross_atn_x = self.cross_attention.forward_cross_attention(addnorm1, enc_output, enc_output, src_mask)\n",
        "        addnorm2 = self.add_norm2(addnorm1, cross_atn_x)\n",
        "        ffnn_x = self.ff_network(addnorm2)\n",
        "        decoder_output = self.add_norm3(addnorm2, ffnn_x)\n",
        "\n",
        "        return decoder_output"
      ],
      "metadata": {
        "id": "fr7xGIhabIwg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder**"
      ],
      "metadata": {
        "id": "yCjKud8tyHFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,num_dec_layers, input_dim, d_model, num_heads, d_ffn):\n",
        "      super().__init__()\n",
        "      self.layers = nn.ModuleList([DecodingLayer(input_dim, d_model, num_heads, d_ffn) for _ in range(num_dec_layers)])\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask = False, tgt_mask = False):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "y4PGNQHqx7go"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer Structure**\n",
        "\n",
        "```\n",
        "structuring encoder,decoder\n",
        "```"
      ],
      "metadata": {
        "id": "dtD3jhph0pLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab, src_vocab_size, tgt_vocab_size, input_dim, d_model, num_heads, d_ffn, num_enc_layers = 1, num_dec_layers = 1,):\n",
        "      super().__init__()\n",
        "\n",
        "      self.encoder = Encoder(num_enc_layers = num_enc_layers, input_dim = input_dim, d_model = d_model, num_heads = num_heads, d_ffn = d_ffn)\n",
        "      self.decoder = Decoder(num_dec_layers= num_dec_layers, input_dim = input_dim, d_model = d_model, num_heads = num_heads, d_ffn = d_ffn)\n",
        "\n",
        "      self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "      self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "      self.final_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "      self.start_token = vocab['<START>']\n",
        "      self.end_token = vocab['<END>']\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.src_embedding(src)\n",
        "        tgt = self.tgt_embedding(tgt)\n",
        "\n",
        "        enc_output = self.encoder(src)\n",
        "        dec_output = self.decoder(tgt, enc_output, tgt_mask = True)\n",
        "\n",
        "        out = self.final_linear(dec_output)\n",
        "\n",
        "        out = nn.functional.softmax(out, dim = -1)\n",
        "        return out\n",
        "\n",
        "    def encode(self, src):\n",
        "        enc_out = self.encoder(src)\n",
        "\n",
        "        return enc_out\n",
        "\n",
        "    def decode(self, tgt, memory):\n",
        "        dec_out = self.decoder(tgt, memory, tgt_mask = True)\n",
        "        dec_out = self.final_linear(dec_out)\n",
        "        return nn.functional.softmax(dec_out, dim = -1)\n",
        "\n",
        "    def generate(self, src, max_len):\n",
        "      memory = self.encode(src)\n",
        "      ys = torch.ones([[1],[1]]).fill_(self.start_token).type_as(src.data)\n",
        "      for _ in range(max_len - 1):\n",
        "        out = self.decode(ys, memory)\n",
        "        next_word = out[:, -1].argmax(dim=-1)\n",
        "        ys = torch.cat([ys, next_word.unsqueeze(1)], dim = 1)\n",
        "        if next_word == self.end_token:\n",
        "          break\n",
        "      return ys\n"
      ],
      "metadata": {
        "id": "mSsnGP-w0JgS"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}